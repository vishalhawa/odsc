{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"2.1.0\"\n",
      "[1] ‘0.9’\n"
     ]
    }
   ],
   "source": [
    "library(tensorflow)\n",
    "library(tfdatasets)\n",
    "library(dplyr)\n",
    "library(glue)\n",
    "library(data.table)\n",
    "library(ggplot2)\n",
    "library(keras)\n",
    "library(tfprobability)\n",
    "library(stringr)\n",
    "source('utils.R')\n",
    "\n",
    "print(tf$version$VERSION)\n",
    "print(tfp_version() ) ## version checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading objects:\n",
      "  df_paths\n",
      "  train_paths\n",
      "  test_paths\n",
      "\n",
      "    0     1 \n",
      "12124  5479 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$embedding_V1\n",
       "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='V1', vocabulary_list=('channel_0', 'channel_1', 'channel_2', 'channel_3', 'channel_4', 'channel_5', 'channel_6', 'channel_7', 'channel_8'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
       "\n",
       "$embedding_V2\n",
       "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='V2', vocabulary_list=('channel_0', 'channel_1', 'channel_2', 'channel_3', 'channel_4', 'channel_5', 'channel_6', 'channel_7', 'channel_8'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
       "\n",
       "$embedding_V3\n",
       "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='V3', vocabulary_list=('channel_0', 'channel_1', 'channel_2', 'channel_3', 'channel_4', 'channel_5', 'channel_6', 'channel_7', 'channel_8'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
       "\n",
       "$embedding_V4\n",
       "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='V4', vocabulary_list=('channel_0', 'channel_1', 'channel_2', 'channel_3', 'channel_4', 'channel_5', 'channel_6', 'channel_7', 'channel_8'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n",
       "\n",
       "$embedding_V5\n",
       "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='V5', vocabulary_list=('channel_0', 'channel_1', 'channel_2', 'channel_3', 'channel_4', 'channel_5', 'channel_6', 'channel_7', 'channel_8'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load data Channel Dataset-----------------------------------------------------\n",
    "load(file = 'df_paths.rdata',verbose = T)\n",
    "\n",
    "encoded_size <- 2L  ## number of dims - Bivariate\n",
    "ev_shape <-  1L\n",
    "batch_size = 32\n",
    "\n",
    "latent_dim <- 2L\n",
    "n_gru = 7\n",
    "ds = train_paths[ str_detect(str_c(V1,V2,V3,V4,V5),'1.*(5|7)')==target]\n",
    "print(table(ds$target))\n",
    "\n",
    "ft_spec <- ds %>%\n",
    "  select(-customer_id,-path_no,-path,) %>%\n",
    "  feature_spec(target ~ .) %>%\n",
    "  step_categorical_column_with_vocabulary_list(starts_with(\"V\"),vocabulary_list = list('channel_0', 'channel_1', 'channel_2', 'channel_3', 'channel_4', 'channel_5', 'channel_6', 'channel_7', 'channel_8')) %>%\n",
    "  step_embedding_column(starts_with(\"V\"), dimension = function(vocab_size) as.integer(sqrt(vocab_size) + 1)   ## dim=4\n",
    "  ) %>%\n",
    "  fit()\n",
    "\n",
    "ft_spec$dense_features() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_ts <- function(name = NULL,ngru=n_gru) {\n",
    "  keras_model_custom(name = name, function(self) {\n",
    "    self$dense_features <- layer_dense_features(feature_columns=ft_spec$dense_features()) ## ft_spec\n",
    "    self$reshape = layer_reshape(target_shape= list(5,4)) \n",
    "    self$gru1  =   (layer=layer_gru(units = ngru, dropout=0.2 ,recurrent_dropout = 0.1,return_sequences =TRUE) )\n",
    "    self$gru2  =   (layer=layer_gru(units = ngru, dropout=0.2 ,recurrent_dropout = 0.1,return_sequences =FALSE) )\n",
    "    self$dense <- layer_dense(units = 2 * latent_dim)\n",
    "    function (inputs, mask = NULL) {\n",
    "      x <- inputs[[1]]  ## x, the input, is of size (batch_size, max_length_input) or timesteps Tx\n",
    "      dense_out = x %>%  \n",
    "        self$dense_features()  %>% # shape=(batch, col*embed)\n",
    "        self$reshape() %>% \n",
    "        self$gru1(initial_state = inputs[[2]] ) %>%  \n",
    "        self$gru2(initial_state = inputs[[2]] ) %>%   ## shape: 100/5/5 and 100/5\n",
    "        self$dense() \n",
    "      tfd_multivariate_normal_diag(loc = dense_out[, 1:encoded_size],\n",
    "                                   scale_diag = tf$nn$softplus(dense_out[, (encoded_size + 1):(2 * encoded_size)] + 1e-5))  \n",
    "    }\n",
    "  })\n",
    "}\n",
    "\n",
    "decoder_model_ts <- function(name = NULL) {\n",
    "  keras_model_custom(name = name, function(self) {\n",
    "    self$dense1 <- layer_dense(units = 7 * 7 , activation = \"relu\") ## \n",
    "    self$dense2 <- layer_dense(units = 1,activation = \"sigmoid\")  ## \n",
    "    function (x, mask = NULL) {\n",
    "      x =  x %>%\n",
    "        self$dense1() %>%\n",
    "        self$dense2() \n",
    "      x\n",
    "    }\n",
    "  })\n",
    "}\n",
    "compute_kl_loss <- function(latent_prior, approx_posterior, approx_posterior_sample) {\n",
    "  kl_div <- approx_posterior$log_prob(approx_posterior_sample) -  latent_prior$log_prob(approx_posterior_sample)\n",
    "  avg_kl_div <- tf$reduce_mean(kl_div)\n",
    "  abs(avg_kl_div)\n",
    "}\n",
    "\n",
    "latent_prior <- tfd_multivariate_normal_diag(  loc  = tf$zeros(list(encoded_size)),  scale_identity_multiplier = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"training loop..\"\n"
     ]
    }
   ],
   "source": [
    "## Training Loop -----------------------------------------------------------\n",
    "print('training loop..')\n",
    "optimizer <- tf$optimizers$Adam()\n",
    "decoder_ts_iso <- decoder_model_ts()\n",
    "encoder_ts_iso <- encoder_model_ts()\n",
    "\n",
    "encoder_init_hidden <- k_zeros(c(batch_size, n_gru), dtype='float32') \n",
    "\n",
    "ch_dataset = tensor_slices_dataset(ds) %>%   dataset_batch(batch_size = batch_size,drop_remainder=T) \n",
    "num_epochs <- 10\n",
    "wt_crossentropy_loss = 4\n",
    "wt_kl_loss = 1\n",
    "batches_num <-  nrow(ds) / batch_size\n",
    "num_samples = 10000\n",
    "\n",
    "for (epoch in seq_len(num_epochs)) {\n",
    "  iter <-   make_iterator_one_shot(ch_dataset)\n",
    "  total_loss_kl <- total_loss_nll <- total_loss <- 0\n",
    "  \n",
    "  until_out_of_range({\n",
    "    x <-  iterator_get_next(iter)\n",
    "    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n",
    "      approx_posterior = encoder_ts_iso( list(x,encoder_init_hidden))\n",
    "      approx_posterior_sample <-   approx_posterior %>% tfd_sample(num_samples)  %>% k_mean(axis = 1)\n",
    "      preds <- decoder_ts_iso(approx_posterior_sample) \n",
    "      nll <-   wt_crossentropy_loss*tf$nn$sigmoid_cross_entropy_with_logits(logits = preds, labels = tf$reshape(x$target,shape = preds$shape) )\n",
    "      kl_loss <-  wt_kl_loss* compute_kl_loss(  latent_prior,  approx_posterior,  approx_posterior_sample ) \n",
    "      loss <- (kl_loss + nll)\n",
    "    }) ## batch  \n",
    "    encoder_gradients <- tape$gradient(loss, encoder_ts_iso$variables)\n",
    "    decoder_gradients <- tape$gradient(loss, decoder_ts_iso$variables)\n",
    "    \n",
    "    optimizer$apply_gradients(  purrr::transpose(list(encoder_gradients, encoder_ts_iso$variables))   )\n",
    "    optimizer$apply_gradients(  purrr::transpose(list(decoder_gradients, decoder_ts_iso$variables))   )\n",
    "    total_loss <- total_loss + loss\n",
    "    total_loss_nll <- total_loss_nll + nll\n",
    "    total_loss_kl <- total_loss_kl + kl_loss\n",
    "  })\n",
    "  print(\n",
    "    glue(\n",
    "      \"Losses (epoch): {epoch}:\",\n",
    "      \"  {mean((as.numeric(total_loss_nll))) %>% round(2)} nll\",\n",
    "      \"  {(as.numeric(total_loss_kl)) %>% round(2)} kl\",\n",
    "      \"  {mean((as.numeric(total_loss))) %>% round(2)} total\"\n",
    "    )\n",
    "  )\n",
    "  if (epoch %% 5 == 0) show_space(epoch,size = 15000,dataset=ch_dataset,pre='iso',encoder = encoder_ts_iso)\n",
    "}  ## train loop \n",
    "\n",
    "## test space -----------------------------------------------------\n",
    "\n",
    "show_space('test',size = 2000, pre='_iso',encoder = encoder_ts_iso,\n",
    "           dataset=tensor_slices_dataset(test_paths) %>%   dataset_batch(batch_size = batch_size,drop_remainder=T) )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
